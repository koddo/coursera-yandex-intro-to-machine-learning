---
layout: page
title: 01b — Логические методы классификации
permalink: /week-01b/
---

<br />
<br />
<br />
<br />
<br />

## Решающие деревья

![image]({{ site.baseurl }}/images/20160128-1432-67856Zj1.screenshot.png){: .slide }

![image]({{ site.baseurl }}/images/20160128-1436-67856LtE.screenshot.png){: .slide }

![image]({{ site.baseurl }}/images/20160128-1437-67856Y3K.screenshot.png){: .slide }

![image]({{ site.baseurl }}/images/20160128-1439-67856lBR.screenshot.png){: .slide }

<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />


## Алгоритм построения решающего дерева

![image]({{ site.baseurl }}/images/20160128-1517-67856_Vd.screenshot.png){: .slide }

![image]({{ site.baseurl }}/images/20160128-1519-67856Mgj.screenshot.png){: .slide }

критерий Джини показывает, сколько есть пар объектов лежащий в одном и том же классе, которые сместе идут либо в левую дочернюю вершину, либо в правую дочернюю вершину <br />
у эти объектов должны совпадать метки классов и совпадать значения предикатов <br />
 <br />
можно придумать и противоположную стратегию, когда мы подсчитываем число пар объектов, которые лежали в разных классах, и они ушли в разные ветки <br />
один пошел налево, другой на право <br />
то есть, если критерий Джини меряет, насколько часто объекты одних классов объединяются, то критерий Донского меряет то, насколько данный предикат обладает способностью разделять объекты разных классов <br />
 <br />
можно проводить эксперименты и смотреть, в каких задачах работает лучше один критерий, в каких работает лучше другой <br />
они строят немного по-разному деревья <br />
 <br />
есть еще один --- энтропийный критерий с достаточно сложной формулой, которая вытекает из теории информации <br />
мы не будем ее разбирать подробно <br />
на практике оказывается, что этот критерий очень похож на критерий Джини <br />
и работает примерно так же <br />

<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />

## Обработка пропусков. Достоинства и недостатки решающих деревьев.

![image]({{ site.baseurl }}/images/20160128-1602-67856Zqp.screenshot.png){: .slide }

![image]({{ site.baseurl }}/images/20160128-1604-67856m0v.screenshot.png){: .slide }

мы можем по-разному задавать множество предикатов, из которого мы выбираем предикаты во внутренних вершинах <br />
вплоть до того, что это могут быть какие-то другие модели классификации -- главное, чтобы они давали ответ да или нет <br />
 <br />
недостатки вытекают из того, что это жадная стратегия <br />
решения о ветвлении, которые принимаются в каждой внутренней вершине -- они локально-оптимальны, но они не оптимальны глобально <br />
мы всегда могли бы полным пребором найти гораздо более компактное решающее дерево, которое решает задачу, может быть, даже с меньшим количеством ошибок <br />
но, поскольку принимая решения о ветвлении во внутренней вершине мы не можем заглянуть далее и понять, насколько хорошо будет идти процесс ветвления дальше, то происходит серия неоптимальных решений о постановке предикатов во внутренних вершинах <br />
а когда мы доходим до вершин, близких к терминальным, мы вообще имеем дело с высокой фрагментацией выборки <br />
туда доходит небольшое число объектов, и решение о том, какой предикат поместить во внутреннюю вершину, или к какому классу отнести объект в терминальной вершине -- эти решения становятся статистически ненадежными <br />
они принимаются по выборкам малого объема <br />
 <br />
ну и наконец, данная процедура очень чувствительна и к составу выборки, и к шумовым признакам <br />
можно показать, что, например, изъятие одного объекта из обучающей выборки часто приводит к тому, что дерево строится совсем другое <br />
в какой-то вершине произошел выбор другого предиката, и дальше все поддерево будет строиться совсем по-другому <br />
 <br />
еще раз, эти недостатки вытекают из того, что алгоритм жадный <br />

<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />

## Способы устранения недостатков решающих деревьев

![image]({{ site.baseurl }}/images/20160128-1619-67856lIF.screenshot.png){: .slide }

известный пример <br />
синтетическая задача классификации <br />
classification of XOR problem <br />
это пример задачи, которая не решается, например, линейным классификатором <br />
выборка не может быть разделена прямой на два класса без ошибок <br />
 <br />
решающее дерево глубины два легко разделяет эту выборку <br />
но, однако, нетривиально догадаться, особенно для жадного алгоритма, что на первом шаге деления надо просто произвольным образом провести прямую -- вертиальную посередине или горизонтальную посередине <br />
и тогда на втором шаге у нас выборка разделится без ошибок <br />
жадный алгоритм этого не видит <br />
жадный алгоритм на первом же шаге попытается отделить два объекта одного из классов <br />
и весь процесс построения дерева пойдет так, что оптимальное построение двухуровневого дерева будет уже невозможно <br />
строится такое дерево из четырех уровней, которое делит выборку не так, как нам хотелось бы <br />

![image]({{ site.baseurl }}/images/20160128-1631-67856ySL.screenshot.png){: .slide }

![image]({{ site.baseurl }}/images/20160128-1634-67856_cR.screenshot.png){: .slide }

![image]({{ site.baseurl }}/images/20160128-1639-67856Zxd.screenshot.png){: .slide }

Резюме:

![image]({{ site.baseurl }}/images/20160128-1641-67856zFq.screenshot.png){: .slide }

плохо работают на выборках с большим количеством признаков

Одна вершина дерева может учитывать лишь один признак — значит, для полноценной работы на выборках с десятками тысяч признаков дереву понадобятся десятки тысяч вершин. Чтобы такое дерево не переобучилось, нужна огромная выборка; более того, обучение такого дерева будет очень трудозатратным. Решающие деревья скорее подходят для выборок с небольшим числом признаков.

<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />

## Quiz: Решающие деревья

![image]({{ site.baseurl }}/images/20160128-1650-67856AQw.screenshot.png){: .slide }

![image]({{ site.baseurl }}/images/20160128-1650-67856Na2.screenshot.png){: .slide }






